{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf579c9",
   "metadata": {},
   "source": [
    "## Dataset Uploading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6024c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.8.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting black>=24.10.0 (from kaggle)\n",
      "  Downloading black-25.11.0-cp312-cp312-win_amd64.whl.metadata (85 kB)\n",
      "Requirement already satisfied: bleach in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Collecting kagglesdk (from kaggle)\n",
      "  Downloading kagglesdk-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy>=1.15.0 (from kaggle)\n",
      "  Downloading mypy-1.19.0-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\tauhid\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (75.1.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\tauhid\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Collecting types-requests (from kaggle)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting types-tqdm (from kaggle)\n",
      "  Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from black>=24.10.0->kaggle) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from black>=24.10.0->kaggle) (1.0.0)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\tauhid\\appdata\\roaming\\python\\python312\\site-packages (from black>=24.10.0->kaggle) (25.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from black>=24.10.0->kaggle) (0.10.3)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\tauhid\\appdata\\roaming\\python\\python312\\site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
      "Collecting pytokens>=0.3.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.6.0 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from mypy>=1.15.0->kaggle) (4.11.0)\n",
      "Collecting librt>=0.6.2 (from mypy>=1.15.0->kaggle)\n",
      "  Downloading librt-0.7.2-cp312-cp312-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: webencodings in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tauhid\\anaconda3\\lib\\site-packages (from requests->kaggle) (2025.11.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\tauhid\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading kaggle-1.8.2-py3-none-any.whl (256 kB)\n",
      "Downloading black-25.11.0-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.5/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.5/1.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 882.6 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 882.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 662.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 662.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 697.4 kB/s eta 0:00:00\n",
      "Downloading mypy-1.19.0-cp312-cp312-win_amd64.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   -- ------------------------------------- 0.5/10.1 MB 155.4 kB/s eta 0:01:02\n",
      "   --- ------------------------------------ 0.8/10.1 MB 129.1 kB/s eta 0:01:13\n",
      "   --- ------------------------------------ 0.8/10.1 MB 129.1 kB/s eta 0:01:13\n",
      "   --- ------------------------------------ 0.8/10.1 MB 129.1 kB/s eta 0:01:13\n",
      "   --- ------------------------------------ 0.8/10.1 MB 129.1 kB/s eta 0:01:13\n",
      "   --- ------------------------------------ 0.8/10.1 MB 129.1 kB/s eta 0:01:13\n",
      "   --- ------------------------------------ 0.8/10.1 MB 129.1 kB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 148.5 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 148.5 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 148.5 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 148.5 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 148.5 kB/s eta 0:01:02\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 148.5 kB/s eta 0:01:02\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 158.3 kB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 158.3 kB/s eta 0:00:56\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 158.3 kB/s eta 0:00:56\n",
      "   ------ --------------------------------- 1.6/10.1 MB 180.4 kB/s eta 0:00:48\n",
      "   ------ --------------------------------- 1.6/10.1 MB 180.4 kB/s eta 0:00:48\n",
      "   ------- -------------------------------- 1.8/10.1 MB 203.4 kB/s eta 0:00:41\n",
      "   ------- -------------------------------- 1.8/10.1 MB 203.4 kB/s eta 0:00:41\n",
      "   -------- ------------------------------- 2.1/10.1 MB 225.0 kB/s eta 0:00:36\n",
      "   -------- ------------------------------- 2.1/10.1 MB 225.0 kB/s eta 0:00:36\n",
      "   --------- ------------------------------ 2.4/10.1 MB 246.7 kB/s eta 0:00:32\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 267.7 kB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 267.7 kB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 284.4 kB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 284.4 kB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 284.4 kB/s eta 0:00:26\n",
      "   ------------ --------------------------- 3.1/10.1 MB 292.5 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 3.1/10.1 MB 292.5 kB/s eta 0:00:24\n",
      "   ------------ --------------------------- 3.1/10.1 MB 292.5 kB/s eta 0:00:24\n",
      "   ------------- -------------------------- 3.4/10.1 MB 300.5 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 3.4/10.1 MB 300.5 kB/s eta 0:00:23\n",
      "   -------------- ------------------------- 3.7/10.1 MB 312.5 kB/s eta 0:00:21\n",
      "   -------------- ------------------------- 3.7/10.1 MB 312.5 kB/s eta 0:00:21\n",
      "   -------------- ------------------------- 3.7/10.1 MB 312.5 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 3.9/10.1 MB 315.3 kB/s eta 0:00:20\n",
      "   --------------- ------------------------ 3.9/10.1 MB 315.3 kB/s eta 0:00:20\n",
      "   --------------- ------------------------ 3.9/10.1 MB 315.3 kB/s eta 0:00:20\n",
      "   --------------- ------------------------ 3.9/10.1 MB 315.3 kB/s eta 0:00:20\n",
      "   --------------- ------------------------ 3.9/10.1 MB 315.3 kB/s eta 0:00:20\n",
      "   --------------- ------------------------ 3.9/10.1 MB 315.3 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 305.8 kB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 271.7 kB/s eta 0:00:21\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------ --------------------- 4.7/10.1 MB 247.8 kB/s eta 0:00:22\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   ------------------- -------------------- 5.0/10.1 MB 230.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   -------------------- ------------------- 5.2/10.1 MB 213.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 5.5/10.1 MB 204.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 5.5/10.1 MB 204.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 5.5/10.1 MB 204.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 5.5/10.1 MB 204.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 5.5/10.1 MB 204.7 kB/s eta 0:00:23\n",
      "   --------------------- ------------------ 5.5/10.1 MB 204.7 kB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 205.0 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 205.0 kB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 205.0 kB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 6.0/10.1 MB 210.3 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 6.0/10.1 MB 210.3 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 6.0/10.1 MB 210.3 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 6.0/10.1 MB 210.3 kB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 6.0/10.1 MB 210.3 kB/s eta 0:00:20\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------ --------------- 6.3/10.1 MB 212.0 kB/s eta 0:00:19\n",
      "   ------------------------- -------------- 6.6/10.1 MB 210.2 kB/s eta 0:00:17\n",
      "   ------------------------- -------------- 6.6/10.1 MB 210.2 kB/s eta 0:00:17\n",
      "   ------------------------- -------------- 6.6/10.1 MB 210.2 kB/s eta 0:00:17\n",
      "   ------------------------- -------------- 6.6/10.1 MB 210.2 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 6.8/10.1 MB 215.5 kB/s eta 0:00:16\n",
      "   -------------------------- ------------- 6.8/10.1 MB 215.5 kB/s eta 0:00:16\n",
      "   -------------------------- ------------- 6.8/10.1 MB 215.5 kB/s eta 0:00:16\n",
      "   --------------------------- ------------ 7.1/10.1 MB 219.8 kB/s eta 0:00:14\n",
      "   --------------------------- ------------ 7.1/10.1 MB 219.8 kB/s eta 0:00:14\n",
      "   --------------------------- ------------ 7.1/10.1 MB 219.8 kB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 7.3/10.1 MB 234.1 kB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 7.3/10.1 MB 234.1 kB/s eta 0:00:12\n",
      "   ------------------------------ --------- 7.6/10.1 MB 239.4 kB/s eta 0:00:11\n",
      "   ------------------------------ --------- 7.6/10.1 MB 239.4 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 7.9/10.1 MB 245.1 kB/s eta 0:00:10\n",
      "   ------------------------------- -------- 7.9/10.1 MB 245.1 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 8.1/10.1 MB 251.1 kB/s eta 0:00:08\n",
      "   --------------------------------- ------ 8.4/10.1 MB 257.3 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 8.4/10.1 MB 257.3 kB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 8.7/10.1 MB 263.0 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 8.9/10.1 MB 271.9 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 9.2/10.1 MB 278.6 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 9.4/10.1 MB 285.0 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 9.4/10.1 MB 285.0 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 9.7/10.1 MB 289.9 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 9.7/10.1 MB 289.9 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 9.7/10.1 MB 289.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  10.0/10.1 MB 298.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 301.1 kB/s eta 0:00:00\n",
      "Downloading kagglesdk-0.1.13-py3-none-any.whl (159 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl (24 kB)\n",
      "Downloading librt-0.7.2-cp312-cp312-win_amd64.whl (54 kB)\n",
      "Downloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: types-requests, pytokens, librt, types-tqdm, mypy, kagglesdk, black, kaggle\n",
      "  Attempting uninstall: mypy\n",
      "    Found existing installation: mypy 1.11.2\n",
      "    Uninstalling mypy-1.11.2:\n",
      "      Successfully uninstalled mypy-1.11.2\n",
      "  Attempting uninstall: black\n",
      "    Found existing installation: black 24.8.0\n",
      "    Uninstalling black-24.8.0:\n",
      "      Successfully uninstalled black-24.8.0\n",
      "Successfully installed black-25.11.0 kaggle-1.8.2 kagglesdk-0.1.13 librt-0.7.2 mypy-1.19.0 pytokens-0.3.0 types-requests-2.32.4.20250913 types-tqdm-4.67.0.20250809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.5.1 requires ipython!=8.17.1,<9.0.0,>=8.13.0; python_version > \"3.8\", but you have ipython 9.6.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a39f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "shutil.copy(\"kaggle.json\", kaggle_dir)\n",
    "\n",
    "\n",
    "os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 600)\n",
    "\n",
    "print(\"Kaggle API token configured successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1f5f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
      "License(s): other\n",
      "Downloading sentiment140.zip to c:\\Users\\Tauhid\\Desktop\\Jupyter Notebook\\SentimentAnalysis\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.9M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/80.9M [00:01<02:06, 664kB/s]\n",
      "  2%|▏         | 2.00M/80.9M [00:01<01:02, 1.32MB/s]\n",
      "  4%|▎         | 3.00M/80.9M [00:01<00:39, 2.08MB/s]\n",
      "  6%|▌         | 5.00M/80.9M [00:02<00:19, 4.06MB/s]\n",
      "  9%|▊         | 7.00M/80.9M [00:02<00:13, 5.95MB/s]\n",
      " 11%|█         | 9.00M/80.9M [00:02<00:09, 8.24MB/s]\n",
      " 14%|█▎        | 11.0M/80.9M [00:02<00:07, 10.2MB/s]\n",
      " 16%|█▌        | 13.0M/80.9M [00:02<00:06, 11.5MB/s]\n",
      " 19%|█▊        | 15.0M/80.9M [00:02<00:05, 13.1MB/s]\n",
      " 21%|██        | 17.0M/80.9M [00:02<00:04, 13.8MB/s]\n",
      " 23%|██▎       | 19.0M/80.9M [00:02<00:04, 15.3MB/s]\n",
      " 26%|██▌       | 21.0M/80.9M [00:03<00:04, 14.6MB/s]\n",
      " 28%|██▊       | 23.0M/80.9M [00:03<00:03, 15.7MB/s]\n",
      " 31%|███       | 25.0M/80.9M [00:03<00:03, 16.2MB/s]\n",
      " 33%|███▎      | 27.0M/80.9M [00:03<00:03, 16.0MB/s]\n",
      " 36%|███▌      | 29.0M/80.9M [00:03<00:03, 16.6MB/s]\n",
      " 38%|███▊      | 31.0M/80.9M [00:03<00:03, 16.1MB/s]\n",
      " 41%|████      | 33.0M/80.9M [00:03<00:03, 16.7MB/s]\n",
      " 43%|████▎     | 35.0M/80.9M [00:04<00:02, 16.4MB/s]\n",
      " 46%|████▌     | 37.0M/80.9M [00:04<00:02, 16.8MB/s]\n",
      " 48%|████▊     | 39.0M/80.9M [00:04<00:02, 16.1MB/s]\n",
      " 52%|█████▏    | 42.0M/80.9M [00:04<00:02, 16.7MB/s]\n",
      " 54%|█████▍    | 44.0M/80.9M [00:04<00:02, 17.3MB/s]\n",
      " 57%|█████▋    | 46.0M/80.9M [00:04<00:02, 16.2MB/s]\n",
      " 61%|██████    | 49.0M/80.9M [00:04<00:01, 17.0MB/s]\n",
      " 63%|██████▎   | 51.0M/80.9M [00:04<00:01, 17.2MB/s]\n",
      " 65%|██████▌   | 53.0M/80.9M [00:05<00:01, 16.8MB/s]\n",
      " 68%|██████▊   | 55.0M/80.9M [00:05<00:01, 17.4MB/s]\n",
      " 70%|███████   | 57.0M/80.9M [00:05<00:01, 16.5MB/s]\n",
      " 73%|███████▎  | 59.0M/80.9M [00:05<00:01, 16.8MB/s]\n",
      " 75%|███████▌  | 61.0M/80.9M [00:05<00:01, 14.3MB/s]\n",
      " 78%|███████▊  | 63.0M/80.9M [00:05<00:01, 13.5MB/s]\n",
      " 83%|████████▎ | 67.0M/80.9M [00:06<00:00, 14.9MB/s]\n",
      " 88%|████████▊ | 71.0M/80.9M [00:06<00:00, 15.6MB/s]\n",
      " 91%|█████████▏| 74.0M/80.9M [00:06<00:00, 15.6MB/s]\n",
      " 96%|█████████▋| 78.0M/80.9M [00:06<00:00, 16.2MB/s]\n",
      "100%|██████████| 80.9M/80.9M [00:06<00:00, 12.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d kazanova/sentiment140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934d24f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zip_path = \"sentiment140.zip\"   \n",
    "\n",
    "with ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"sentiment140\")  \n",
    "\n",
    "print(\"Dataset extracted successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322545a9",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc88cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd6bc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tauhid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdceac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read dataset\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "tweets_df = pd.read_csv(\"sentiment140/training.1600000.processed.noemoticon.csv\",names=column_names, encoding='ISO-8859-1',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf57e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95c7c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b95e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.isnull().sum()\n",
    "\n",
    "## No missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9499c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['target'].value_counts()\n",
    "\n",
    "\n",
    "# 0 - Negative Sentiment\n",
    "# 4 - Positive Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709375ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.replace({'target': {4:1}}, inplace=True)\n",
    "## Converted 4 to 1 for positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9002b269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3b6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5e9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content) \n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc58e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['stemmed_text'] = tweets_df['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6348440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>switchfoot http twitpic com zl awww bummer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                        stemmed_text  \n",
       "0  switchfoot http twitpic com zl awww bummer sho...  \n",
       "1  upset updat facebook text might cri result sch...  \n",
       "2  kenichan dive mani time ball manag save rest g...  \n",
       "3                    whole bodi feel itchi like fire  \n",
       "4                      nationwideclass behav mad see  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "323b1e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          switchfoot http twitpic com zl awww bummer sho...\n",
      "1          upset updat facebook text might cri result sch...\n",
      "2          kenichan dive mani time ball manag save rest g...\n",
      "3                            whole bodi feel itchi like fire\n",
      "4                              nationwideclass behav mad see\n",
      "                                 ...                        \n",
      "1599995                           woke school best feel ever\n",
      "1599996    thewdb com cool hear old walt interview http b...\n",
      "1599997                         readi mojo makeov ask detail\n",
      "1599998    happi th birthday boo alll time tupac amaru sh...\n",
      "1599999    happi charitytuesday thenspcc sparkschar speak...\n",
      "Name: stemmed_text, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df['stemmed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4bf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating features and target\n",
    "X = tweets_df['stemmed_text'].values\n",
    "y = tweets_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "104b233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot http twitpic com zl awww bummer shoulda got david carr third day'\n",
      " 'upset updat facebook text might cri result school today also blah'\n",
      " 'kenichan dive mani time ball manag save rest go bound' ...\n",
      " 'readi mojo makeov ask detail'\n",
      " 'happi th birthday boo alll time tupac amaru shakur'\n",
      " 'happi charitytuesday thenspcc sparkschar speakinguph h']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "117431da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6484db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "655bf3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000,) (320000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fd7fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COnvert Textual column into numerical using TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a472bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 436713)\t0.27259876264838384\n",
      "  (0, 354543)\t0.3588091611460021\n",
      "  (0, 185193)\t0.5277679060576009\n",
      "  (0, 109306)\t0.3753708587402299\n",
      "  (0, 235045)\t0.41996827700291095\n",
      "  (0, 443066)\t0.4484755317023172\n",
      "  (1, 160636)\t1.0\n",
      "  (2, 109306)\t0.4591176413728317\n",
      "  (2, 124484)\t0.1892155960801415\n",
      "  (2, 407301)\t0.18709338684973031\n",
      "  (2, 129411)\t0.29074192727957143\n",
      "  (2, 406399)\t0.32105459490875526\n",
      "  (2, 433560)\t0.3296595898028565\n",
      "  (2, 77929)\t0.31284080750346344\n",
      "  (2, 443430)\t0.3348599670252845\n",
      "  (2, 266729)\t0.24123230668976975\n",
      "  (2, 409143)\t0.15169282335109835\n",
      "  (2, 178061)\t0.1619010109445149\n",
      "  (2, 150715)\t0.18803850583207948\n",
      "  (2, 132311)\t0.2028971570399794\n",
      "  (2, 288470)\t0.16786949597862733\n",
      "  (3, 406399)\t0.29029991238662284\n",
      "  (3, 158711)\t0.4456939372299574\n",
      "  (3, 151770)\t0.278559647704793\n",
      "  (3, 56476)\t0.5200465453608686\n",
      "  :\t:\n",
      "  (1279996, 318303)\t0.21254698865277744\n",
      "  (1279996, 434014)\t0.27189450523324465\n",
      "  (1279996, 390130)\t0.2206474219107611\n",
      "  (1279996, 373144)\t0.35212500999832036\n",
      "  (1279996, 238077)\t0.5249170684084672\n",
      "  (1279996, 238078)\t0.5606696159563151\n",
      "  (1279997, 5685)\t0.48650358607431304\n",
      "  (1279997, 273084)\t0.4353549002982409\n",
      "  (1279997, 112591)\t0.7574829183045267\n",
      "  (1279998, 412553)\t0.2816582375021589\n",
      "  (1279998, 93795)\t0.21717768937055476\n",
      "  (1279998, 169461)\t0.2659980990397061\n",
      "  (1279998, 124765)\t0.32241752985927996\n",
      "  (1279998, 435463)\t0.2851807874350361\n",
      "  (1279998, 153281)\t0.28378968751027456\n",
      "  (1279998, 156297)\t0.3137096161546449\n",
      "  (1279998, 162047)\t0.34691726958159064\n",
      "  (1279998, 275288)\t0.38703346602729577\n",
      "  (1279998, 385313)\t0.4103285865588191\n",
      "  (1279999, 242268)\t0.19572649660865402\n",
      "  (1279999, 31410)\t0.248792678366695\n",
      "  (1279999, 435572)\t0.31691096877786484\n",
      "  (1279999, 433612)\t0.3607341026233411\n",
      "  (1279999, 135384)\t0.6130934129868719\n",
      "  (1279999, 96224)\t0.5416162421321443\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "155011f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15110)\t0.1719352837797837\n",
      "  (0, 31168)\t0.1624772418052177\n",
      "  (0, 67828)\t0.26800375270827315\n",
      "  (0, 106069)\t0.36555450010904555\n",
      "  (0, 132364)\t0.255254889555786\n",
      "  (0, 138164)\t0.23688292264071406\n",
      "  (0, 171378)\t0.2805816206356074\n",
      "  (0, 271016)\t0.45356623916588285\n",
      "  (0, 279082)\t0.17825180109103442\n",
      "  (0, 388348)\t0.2198507607206174\n",
      "  (0, 398906)\t0.34910438732642673\n",
      "  (0, 409143)\t0.3143047059807971\n",
      "  (0, 420984)\t0.17915624523539805\n",
      "  (1, 6463)\t0.30733520460524466\n",
      "  (1, 15110)\t0.211037449588008\n",
      "  (1, 145393)\t0.575262969264869\n",
      "  (1, 217562)\t0.40288153995289894\n",
      "  (1, 256777)\t0.28751585696559306\n",
      "  (1, 348135)\t0.4739279595416274\n",
      "  (1, 366203)\t0.24595562404108307\n",
      "  (2, 22532)\t0.3532582957477176\n",
      "  (2, 34401)\t0.37916255084357414\n",
      "  (2, 89448)\t0.36340369428387626\n",
      "  (2, 183312)\t0.5892069252021465\n",
      "  (2, 256834)\t0.2564939661498776\n",
      "  :\t:\n",
      "  (319994, 443794)\t0.2782185641032538\n",
      "  (319995, 107868)\t0.33399349737546963\n",
      "  (319995, 109379)\t0.3020896484890833\n",
      "  (319995, 155493)\t0.2770682832971669\n",
      "  (319995, 213324)\t0.2683969144317079\n",
      "  (319995, 232891)\t0.2574127854589077\n",
      "  (319995, 296662)\t0.3992485679384015\n",
      "  (319995, 315813)\t0.2848229914563413\n",
      "  (319995, 324496)\t0.36131679336475747\n",
      "  (319995, 416257)\t0.23816465111736282\n",
      "  (319995, 420984)\t0.22631428606830148\n",
      "  (319995, 444934)\t0.32110928175992615\n",
      "  (319996, 397506)\t0.9101400928717545\n",
      "  (319996, 438709)\t0.4143006291901984\n",
      "  (319997, 98792)\t0.4463892055808332\n",
      "  (319997, 169411)\t0.403381646999604\n",
      "  (319997, 261286)\t0.37323893626855326\n",
      "  (319997, 288421)\t0.48498483387153407\n",
      "  (319997, 349904)\t0.32484594100566083\n",
      "  (319997, 416695)\t0.29458327588067873\n",
      "  (319997, 444770)\t0.2668297951055569\n",
      "  (319998, 130192)\t0.6941927210956169\n",
      "  (319998, 438748)\t0.719789181620468\n",
      "  (319999, 389755)\t0.9577980203954275\n",
      "  (319999, 400636)\t0.2874420848216212\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a261d9f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e80ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Model Performance for Training Set\n",
      "-Accuracy: 0.7987\n",
      "-------------------------------------------------------\n",
      "Model Performance for Testing Set\n",
      "-Accuracy: 0.7767\n",
      "Linear SVM\n",
      "Model Performance for Training Set\n",
      "-Accuracy: 0.8623\n",
      "-------------------------------------------------------\n",
      "Model Performance for Testing Set\n",
      "-Accuracy: 0.7697\n",
      "Naive Bayes\n",
      "Model Performance for Training Set\n",
      "-Accuracy: 0.8200\n",
      "-------------------------------------------------------\n",
      "Model Performance for Testing Set\n",
      "-Accuracy: 0.7558\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "\n",
    "for i in range (len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    ##Training Accuracy\n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    ##Testing Accuracy\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "\n",
    "    \n",
    "    print(\"Model Performance for Training Set\")\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print('-------------------------------------------------------')\n",
    "    print(\"Model Performance for Testing Set\")\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f45d7",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3207dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"C\": [0.01,10],\n",
    "    \"solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"max_iter\": [500, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e35407f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': [0.01, 10], 'solver': ['liblinear', 'lbfgs'], 'max_iter': [500, 1000]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "087abd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using RandomSearch CV\n",
    "randomcv_models = [\n",
    "                    (\"LR\", LogisticRegression(), params),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e75c52ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LR',\n",
       "  LogisticRegression(),\n",
       "  {'C': [0.01, 10],\n",
       "   'solver': ['liblinear', 'lbfgs'],\n",
       "   'max_iter': [500, 1000]})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomcv_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7789c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model_param = {}\n",
    "for name, model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                                param_distributions=params,\n",
    "                                n_iter=10,\n",
    "                                cv=3,\n",
    "                                verbose=2,\n",
    "                                n_jobs=-1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e99110",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.fit(X_train,y_train)\n",
    "model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"----------------Best Param for {model_name}---------------\")\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54b2b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\" : LogisticRegression(  \n",
    "    solver='liblinear',\n",
    "    max_iter=1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03f7f2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Model Performance for Training Set\n",
      "-Accuracy: 0.8102\n",
      "-------------------------------------------------------\n",
      "Model Performance for Testing Set\n",
      "-Accuracy: 0.7780\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    ## Make Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    ##Training Accuracy\n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    ##Testing Accuracy\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "\n",
    "    \n",
    "    print(\"Model Performance for Training Set\")\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print('-------------------------------------------------------')\n",
    "    print(\"Model Performance for Testing Set\")\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a9c447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"best_model.pkl\"\n",
    "pickle.dump(model, open(filename,'wb'))\n",
    "pickle.dump(tfidf, open(\"vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23828ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
